{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MzsUTBxcRUQ-",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-generativeai\n",
      "  Downloading google_generativeai-0.8.4-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
      "  Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting google-api-core (from google-generativeai)\n",
      "  Downloading google_api_core-2.24.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-api-python-client (from google-generativeai)\n",
      "  Downloading google_api_python_client-2.163.0-py2.py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting google-auth>=2.15.0 (from google-generativeai)\n",
      "  Downloading google_auth-2.38.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: protobuf in ./.conda/envs/patafest/lib/python3.12/site-packages (from google-generativeai) (3.20.3)\n",
      "Requirement already satisfied: pydantic in ./.conda/envs/patafest/lib/python3.12/site-packages (from google-generativeai) (2.10.4)\n",
      "Requirement already satisfied: tqdm in ./.conda/envs/patafest/lib/python3.12/site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in ./.conda/envs/patafest/lib/python3.12/site-packages (from google-generativeai) (4.12.2)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core->google-generativeai)\n",
      "  Downloading googleapis_common_protos-1.69.1-py2.py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in ./.conda/envs/patafest/lib/python3.12/site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./.conda/envs/patafest/lib/python3.12/site-packages (from google-auth>=2.15.0->google-generativeai) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./.conda/envs/patafest/lib/python3.12/site-packages (from google-auth>=2.15.0->google-generativeai) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting httplib2<1.dev0,>=0.19.0 (from google-api-python-client->google-generativeai)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai)\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai)\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.conda/envs/patafest/lib/python3.12/site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.conda/envs/patafest/lib/python3.12/site-packages (from pydantic->google-generativeai) (2.27.2)\n",
      "Collecting grpcio<2.0dev,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading grpcio-1.71.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading grpcio_status-1.71.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in ./.conda/envs/patafest/lib/python3.12/site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in ./.conda/envs/patafest/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/envs/patafest/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/envs/patafest/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/envs/patafest/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/envs/patafest/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2024.12.14)\n",
      "Collecting protobuf (from google-generativeai)\n",
      "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Downloading google_generativeai-0.8.4-py3-none-any.whl (175 kB)\n",
      "Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_api_core-2.24.2-py3-none-any.whl (160 kB)\n",
      "Downloading google_auth-2.38.0-py2.py3-none-any.whl (210 kB)\n",
      "Downloading google_api_python_client-2.163.0-py2.py3-none-any.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading googleapis_common_protos-1.69.1-py2.py3-none-any.whl (293 kB)\n",
      "Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Downloading grpcio-1.71.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio_status-1.71.0-py3-none-any.whl (14 kB)\n",
      "Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Installing collected packages: uritemplate, rsa, protobuf, httplib2, grpcio, proto-plus, googleapis-common-protos, google-auth, grpcio-status, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google-generativeai\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "Successfully installed google-ai-generativelanguage-0.6.15 google-api-core-2.24.2 google-api-python-client-2.163.0 google-auth-2.38.0 google-auth-httplib2-0.2.0 google-generativeai-0.8.4 googleapis-common-protos-1.69.1 grpcio-1.71.0 grpcio-status-1.71.0 httplib2-0.22.0 proto-plus-1.26.1 protobuf-5.29.3 rsa-4.9 uritemplate-4.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "tPnmNd8RQXor"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"irosva.es.training.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u0bOxJ4mP51G",
    "outputId": "cd31dc98-1a9e-4b00-dae3-9e1fc87052f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 ID  TOPIC  IS_IRONIC  \\\n",
      "0  9c91359304feb0bd108c1cf222c22ab0  TARDÀ          1   \n",
      "1  2bb48c858c36efb5e2b142fbbd8bcb2b  TARDÀ          1   \n",
      "2  eed730fc76f85c2f8309f75068577a3f  TARDÀ          1   \n",
      "3  82a82f9452de616105324b5be595c8c9  TARDÀ          1   \n",
      "4  c9542d7219f0f9ec0ad26fbe86645794  TARDÀ          1   \n",
      "\n",
      "                                             MESSAGE  \n",
      "0  En vez de Joan Tarda van a llamarle “No han ta...  \n",
      "1  Joan Tardà responderá a Vox en catalán....si, ...  \n",
      "2  Que hayan dejado marcharse libre a Joan Tardà ...  \n",
      "3  Para intervenciones, la del diputado de ERC Jo...  \n",
      "4  Ya tenía razón el diputado Joan Tardà con aque...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def remove(text):\n",
    "    return re.sub(r'@\\w+', '', text).strip()\n",
    "df[\"MESSAGE\"] = df[\"MESSAGE\"].astype(str).apply(remove)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "6QImvwx1vOxI",
    "outputId": "1f1ed873-c90b-4c37-b5ce-c15c58777204"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traduciendo: 100%|██████████| 2400/2400 [31:48<00:00,  1.26fila/s]\n",
      "Traduciendo: 100%|██████████| 2400/2400 [27:36<00:00,  1.45fila/s]\n",
      "Traduciendo: 100%|██████████| 2400/2400 [39:57<00:00,  1.00fila/s]\n",
      "Traduciendo: 100%|██████████| 2400/2400 [26:35<00:00,  1.50fila/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hecho\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configurar la clave de API\n",
    "genai.configure(api_key=\"\")\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-2.0-flash-lite\")\n",
    "\n",
    "def traducir_texto(texto, source_lang=\"es\", target_lang=\"ca\"):\n",
    "    \"\"\"Traduce un texto usando la API de Gemini.\"\"\"\n",
    "    prompt = f\"Traduce el siguiente texto del {source_lang} al {target_lang}. Devuelve solo el texto traducido, sin introducción o aclaraciones. Texto: {texto}\"\n",
    "    respuesta = model.generate_content(prompt)\n",
    "    return respuesta.text.strip() if respuesta.text else \"\"\n",
    "\n",
    "# Quitar comentario si no ejecutas la celda de filtro de arriba\n",
    "df = pd.read_csv(\"irosva.es.training.csv\")#(\"\")\n",
    "\n",
    "df[\"MESSAGE\"] = df[\"MESSAGE\"].astype(str).apply(remove)\n",
    "\n",
    "df[\"message_ca\"] = [traducir_texto(str(x),\"español\",\"catalán\") for x in tqdm(df[\"MESSAGE\"], desc=\"Traduciendo\", unit=\"fila\")]\n",
    "df[\"message_gl\"] = [traducir_texto(str(x),\"español\",\"gallego\") for x in tqdm(df[\"MESSAGE\"], desc=\"Traduciendo\", unit=\"fila\")]\n",
    "df[\"message_eu\"] = [traducir_texto(str(x),\"español\",\"euskera\") for x in tqdm(df[\"MESSAGE\"], desc=\"Traduciendo\", unit=\"fila\")]\n",
    "df[\"message_pt\"] = [traducir_texto(str(x),\"español\",\"portugués\") for x in tqdm(df[\"MESSAGE\"], desc=\"Traduciendo\", unit=\"fila\")]\n",
    "\n",
    "\n",
    "df.to_csv(\"irosva.es.training_translated.csv\", index=False)\n",
    "\n",
    "print(\"Hecho\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       En comptes de Joan Tarda li diran “No han tard...\n",
       "1       Joan Tardà respondrà a Vox en català....sí, sí...\n",
       "2       Que hagin deixat marxar lliure a Joan Tardà el...\n",
       "3       Per intervencions, la del diputat d'ERC Joan T...\n",
       "4       Ja tenia raó el diputat Joan Tardà amb allò de...\n",
       "                              ...                        \n",
       "2395    Si està preparat per a ser president perquè ha...\n",
       "2396    La supèrbia no té límits... Que està més prepa...\n",
       "2397    Diu que fa tres mesos que canvia bolquers i ja...\n",
       "2398    Jo vaig canviar bolquers des del primer dia i ...\n",
       "2399    Tu a les teves bolquers, aquest ha de ser el t...\n",
       "Name: message_ca, Length: 2400, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"message_ca\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       En vez de Joan Tarda van a chamarlle “Non han ...\n",
       "1       Joan Tardà responderá a Vox en catalán....si, ...\n",
       "2       Que deixasen marchar libre a Joan Tardà eleva ...\n",
       "3       Para intervencións, a do deputado de ERC Joan ...\n",
       "4       Xa tiña razón o deputado Joan Tardà con aquilo...\n",
       "                              ...                        \n",
       "2395    Se está preparado para ser presidente porque l...\n",
       "2396    A soberbia de non ten límites... Que está máis...\n",
       "2397    Di que leva tres meses cambiando cueiros e xa ...\n",
       "2398    Eu cambiei cueiros dende o primeiro día e logo...\n",
       "2399    Ti aos teus cueiros ese debe ser o teu obxectivo.\n",
       "Name: message_gl, Length: 2400, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"message_gl\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vRG1s2uwwyLl",
    "outputId": "4241f526-692b-4224-8baf-9b382d2a2bb0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001\n",
      "['generateMessage', 'countMessageTokens']\n",
      "models/text-bison-001\n",
      "['generateText', 'countTextTokens', 'createTunedTextModel']\n",
      "models/embedding-gecko-001\n",
      "['embedText', 'countTextTokens']\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "['generateContent', 'countTokens']\n",
      "models/gemini-pro-vision\n",
      "['generateContent', 'countTokens']\n",
      "models/gemini-1.5-pro-latest\n",
      "['generateContent', 'countTokens']\n",
      "models/gemini-1.5-pro-001\n",
      "['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-1.5-pro-002\n",
      "['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-1.5-pro\n",
      "['generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-latest\n",
      "['generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-001\n",
      "['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "['generateContent', 'countTokens', 'createTunedModel']\n",
      "models/gemini-1.5-flash\n",
      "['generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-002\n",
      "['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-1.5-flash-8b\n",
      "['createCachedContent', 'generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-8b-001\n",
      "['createCachedContent', 'generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "['createCachedContent', 'generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "['generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "['generateContent', 'countTokens']\n",
      "models/gemini-2.0-flash-exp\n",
      "['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "models/gemini-2.0-flash\n",
      "['generateContent', 'countTokens']\n",
      "models/gemini-2.0-flash-001\n",
      "['generateContent', 'countTokens']\n",
      "models/gemini-2.0-flash-lite-001\n",
      "['generateContent', 'countTokens']\n",
      "models/gemini-2.0-flash-lite\n",
      "['generateContent', 'countTokens']\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "['generateContent', 'countTokens']\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "['generateContent', 'countTokens']\n",
      "models/gemini-2.0-pro-exp\n",
      "['generateContent', 'countTokens']\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "['generateContent', 'countTokens']\n",
      "models/gemini-exp-1206\n",
      "['generateContent', 'countTokens']\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "['generateContent', 'countTokens']\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "['generateContent', 'countTokens']\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "['generateContent', 'countTokens']\n",
      "models/learnlm-1.5-pro-experimental\n",
      "['generateContent', 'countTokens']\n",
      "models/gemma-3-27b-it\n",
      "['generateContent', 'countTokens']\n",
      "models/embedding-001\n",
      "['embedContent']\n",
      "models/text-embedding-004\n",
      "['embedContent']\n",
      "models/gemini-embedding-exp-03-07\n",
      "['embedContent']\n",
      "models/gemini-embedding-exp\n",
      "['embedContent']\n",
      "models/aqa\n",
      "['generateAnswer']\n",
      "models/imagen-3.0-generate-002\n",
      "['predict']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Replace with your actual API key\n",
    "genai.configure(api_key=\"\")\n",
    "\n",
    "models = genai.list_models()  # Get a list of all models\n",
    "\n",
    "# Iterate through the generator object and print the models\n",
    "for model_info in models:\n",
    "    print(model_info.name)  # Print the name of the model\n",
    "    print(model_info.supported_generation_methods)  # Print supported generation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NVZZYiiSyYrF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
